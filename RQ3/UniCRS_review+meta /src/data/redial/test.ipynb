{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "train_review_path = \"/projects/prjs1158/KG/redail/MESE_review/DATA/dataset/redial/nltk/train_conv_idx_to_review_info.pkl\"\n",
    "valid_review_path = \"/projects/prjs1158/KG/redail/MESE_review/DATA/dataset/redial/nltk/valid_conv_idx_to_review_info.pkl\"\n",
    "test_review_path = \"/projects/prjs1158/KG/redail/MESE_review/DATA/dataset/redial/nltk/test_conv_idx_to_review_info.pkl\"\n",
    "\n",
    "train_review_data = pickle.load(open(train_review_path, \"rb\"))\n",
    "valid_review_data = pickle.load(open(valid_review_path, \"rb\"))\n",
    "test_review_data = pickle.load(open(test_review_path, \"rb\"))\n",
    "\n",
    "entity2id_path = \"/projects/prjs1158/KG/redail/MESE_review/DATA/dataset/redial/nltk/entity2id.json\"\n",
    "token2id_path = \"/projects/prjs1158/KG/redail/MESE_review/DATA/dataset/redial/nltk/token2id.json\"\n",
    "entity2id = json.load(open(entity2id_path, 'r'))\n",
    "token2id = json.load(open(token2id_path, 'r'))\n",
    "id2entity = {v: k for k, v in entity2id.items()}\n",
    "id2token = {v: k for k, v in token2id.items()}\n",
    "\n",
    "entity_name_to_review = {}\n",
    "for review_data in [train_review_data, valid_review_data, test_review_data]:\n",
    "    for key in review_data.keys():\n",
    "        for cnt, entity_id in enumerate(review_data[key]['selected_entityIds']):\n",
    "            review = review_data[key]['selected_infoListListInt'][cnt]\n",
    "            # transform review to text\n",
    "            new_review = [id2token[item] for item in review]\n",
    "            new_review = \" \".join(new_review)\n",
    "            entity_name = id2entity[int(entity_id)]\n",
    "            entity_name_to_review[entity_name] = new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/70510/ipykernel_425451/3335391621.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  meta_info = torch.load(meta_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def load_movie_data(csv_path, train_path, valid_path, test_path):\n",
    "    # 首先获取所有在对话中被提到的电影ID\n",
    "    id2link = {}\n",
    "    mentioned_ids, new_id2link = get_mentioned_movie_ids(train_path, id2link)\n",
    "    valid_mentioned_id, new_id2link = get_mentioned_movie_ids(valid_path, new_id2link)\n",
    "    mentioned_ids.update(valid_mentioned_id)\n",
    "    test_mentioned_id, new_id2link = get_mentioned_movie_ids(test_path, new_id2link)\n",
    "    mentioned_ids.update(test_mentioned_id)\n",
    "\n",
    "    # print(mentioned_ids)\n",
    "\n",
    "    items_db = {}\n",
    "    new_item_db = {}\n",
    "\n",
    "    meta_path = '/projects/prjs1158/KG/redail/efficient_unified_crs_place/data/REDIAL/movie_db'\n",
    "\n",
    "    meta_info = torch.load(meta_path)\n",
    "    meta_dict = {}\n",
    "    for key in meta_info.keys():\n",
    "        meta = meta_info[key]\n",
    "        # get the name before [SEP]\n",
    "        title = meta.split('[SEP]')[0].strip()\n",
    "        meta_dict[title] = \" \".join(meta.split('[SEP]')[1:])\n",
    "\n",
    "    name2meta = {}\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cnt = 0\n",
    "        for row in reader:\n",
    "            movie_id = int(row['movieId'])\n",
    "            # 只有当电影ID在对话中被提到过时才保存\n",
    "            if movie_id in mentioned_ids:\n",
    "                # Remove content inside brackets and strip any extra whitespace\n",
    "                import re\n",
    "                new_movie_name = re.sub(r'\\s*\\(.*?\\)\\s*', '', row['movieName']).strip()\n",
    "\n",
    "                # lower case\n",
    "                new_movie_name = new_movie_name.lower()\n",
    "                # print(new_movie_name)\n",
    "                # print(new_movie_name in meta_dict)\n",
    "                if new_movie_name in meta_dict:  \n",
    "                    name2meta[id2link[movie_id]] = meta_dict[new_movie_name]       \n",
    "                    items_db[movie_id] = {\n",
    "                        \"movieName\": new_movie_name,\n",
    "                        'meta': meta_dict[new_movie_name],\n",
    "                        \"nbMentions\": int(row['nbMentions']),\n",
    "                        \"new_item_id\": cnt\n",
    "                    }\n",
    "                    new_item_db[cnt] = {\n",
    "                        \"movieName\": new_movie_name,\n",
    "                        'meta': meta_dict[new_movie_name],}\n",
    "                else:\n",
    "                    name2meta[id2link[movie_id]] = ''\n",
    "                    items_db[movie_id] = {\n",
    "                        \"movieName\": new_movie_name,\n",
    "                        'meta': '',\n",
    "                        \"nbMentions\": int(row['nbMentions']),\n",
    "                        \"new_item_id\": cnt\n",
    "                    }\n",
    "                    new_item_db[cnt] = {\n",
    "                        \"movieName\": new_movie_name,\n",
    "                        'meta': '',}\n",
    "                cnt += 1\n",
    "    # print(items_db)\n",
    "    return items_db, new_item_db, name2meta\n",
    "\n",
    "def get_mentioned_movie_ids(data, id2link):\n",
    "    with open(data, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \"\"\"从对话数据中提取所有被提到的电影ID（格式为@[ID]）\"\"\"\n",
    "    mentioned_ids = set()\n",
    "    # id2link = {}\n",
    "    \n",
    "    for conv in data:\n",
    "        for turn in conv[\"dialog\"]:\n",
    "            text = \" \".join(turn[\"text\"])\n",
    "            # 在文本中查找所有@开头的ID\n",
    "            import re\n",
    "            movie_mentions = re.findall(r'@(\\d+)', text)\n",
    "            mentioned_ids.update(movie_mentions)\n",
    "            for cnt, movie_id in enumerate(movie_mentions):\n",
    "                if cnt < len(turn[\"movies\"]):\n",
    "                    id2link[int(movie_id)] = turn[\"movies\"][cnt]\n",
    "                else:\n",
    "                    id2link[int(movie_id)] = ''\n",
    "    # change to int\n",
    "    mentioned_ids = set([int(x) for x in mentioned_ids])\n",
    "    return mentioned_ids, id2link\n",
    "\n",
    "csv_path = '/projects/prjs1158/KG/redail/MESE_review/DATA/nltk/movies_with_mentions.csv'\n",
    "train_path = '/projects/prjs1158/KG/redail/UniMIND_meta/data/redail/nltk/train_data.json'\n",
    "valid_path = '/projects/prjs1158/KG/redail/UniMIND_meta/data/redail/nltk/valid_data.json'\n",
    "test_path = '/projects/prjs1158/KG/redail/UniMIND_meta/data/redail/nltk/test_data.json'\n",
    "\n",
    "items_db, new_item_db, name2meta = load_movie_data(csv_path, train_path, valid_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://dbpedia.org/resource/Ghostbusters>\n",
      "A junior woman agent is all the FBI cares to spare for this case\n"
     ]
    }
   ],
   "source": [
    "for key in entity_name_to_review.keys():\n",
    "    print(key)\n",
    "    print(entity_name_to_review[key])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1342it [00:00, 3547.28it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_test_data_path = './test_data_dbpedia.jsonl'\n",
    "new_test_data_path = './test_data_dbpedia_review.jsonl'\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open(ori_test_data_path, 'r') as f, open(new_test_data_path, 'w', encoding='utf-8') as fout:\n",
    "    for line in tqdm(f):\n",
    "        dialog = json.loads(line)\n",
    "        messages = dialog['messages']\n",
    "        for message in messages:\n",
    "            # print(message['text'])\n",
    "            if message['entity']:\n",
    "                for entity_name in message['entity']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "            # print(message['text'])\n",
    "            if message['movie']:\n",
    "                for entity_name in message['movie']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "        # write to new file\n",
    "        fout.write(json.dumps(dialog, ensure_ascii=False) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 3042.01it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_test_data_path = './valid_data_dbpedia.jsonl'\n",
    "new_test_data_path = './valid_data_dbpedia_review.jsonl'\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open(ori_test_data_path, 'r') as f, open(new_test_data_path, 'w', encoding='utf-8') as fout:\n",
    "    for line in tqdm(f):\n",
    "        dialog = json.loads(line)\n",
    "        messages = dialog['messages']\n",
    "        for message in messages:\n",
    "            # print(message['text'])\n",
    "            if message['entity']:\n",
    "                for entity_name in message['entity']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "            # print(message['text'])\n",
    "            if message['movie']:\n",
    "                for entity_name in message['movie']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "        # write to new file\n",
    "        fout.write(json.dumps(dialog, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9006it [00:01, 4510.37it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_test_data_path = './train_data_dbpedia.jsonl'\n",
    "new_test_data_path = './train_data_dbpedia_review.jsonl'\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open(ori_test_data_path, 'r') as f, open(new_test_data_path, 'w', encoding='utf-8') as fout:\n",
    "    for line in tqdm(f):\n",
    "        dialog = json.loads(line)\n",
    "        messages = dialog['messages']\n",
    "        for message in messages:\n",
    "            # print(message['text'])\n",
    "            if message['entity']:\n",
    "                for entity_name in message['entity']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "            # print(message['text'])\n",
    "            if message['movie']:\n",
    "                for entity_name in message['movie']:\n",
    "                    if entity_name in entity_name_to_review.keys():\n",
    "                        review = entity_name_to_review[entity_name]\n",
    "                        if entity_name in name2meta.keys():\n",
    "                            meta = name2meta[entity_name]\n",
    "                        else:\n",
    "                            meta = ''\n",
    "                        message['text'] = message['text'] + \" Review: \" + review + \" Meta: \" + meta\n",
    "        \n",
    "        # write to new file\n",
    "        fout.write(json.dumps(dialog, ensure_ascii=False) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
